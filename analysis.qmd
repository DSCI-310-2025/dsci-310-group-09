# Car Acceptability Category Prediction Analysis

## Summary

We aim to develop a Random Forest classification model to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). This prediction is based on categorical attributes such as price, maintenance cost, safety, and seating capacity. The data is sourced from the Car Evaluation dataset found in the UCI Machine Learning Repository (Bohanec, 1998).

Our initial data exploration revealed a significant class imbalance, with "unacceptable" cars dominating the dataset. To improve classification performance, we applied ordinal encoding to categorical features and utilized SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.

Our Random Forest model achieved an impressive overall accuracy of 99% and a strong Kappa statistic of 0.99, indicating robust predictive performance and a strong agreement between the predicted and actual classes. Overall, the model provides a highly accurate and interpretable approach to classifying car acceptability, though further refinements could enhance fairness across all categories.

In summary, this analysis demonstrates how machine learning can be effectively applied to real-world decision-making in the automotive sector.

## Introduction

Evaluating car acceptability is a critical factor in decision-making within the automotive industry. It affects consumer choices regarding vehicle purchases, manufacturers' priorities, and dealership strategies. Car purchases represent one of the most significant financial decisions for households, with affordability being the primary barrier for many buyers. Price plays a crucial role in accessibility, particularly for budget-conscious consumers, such as first-time buyers or individuals in emerging markets.

According to Chiu et al. (2022), price dispersion positively impacts car acceptability. Because car purchases are high-cost, long-term investments, misaligned choices due to information asymmetry—such as undervaluing safety features—can lead to serious financial or safety repercussions (Canada, 2024). For instance, vehicles with poor safety ratings have been linked to higher accident rates. Consequently, Vrkljan et al. (2011) concluded that safety is considered the most important feature when purchasing a vehicle. Automating car evaluations helps buyers efficiently identify optimal vehicles, aligning with the trend toward data-driven consumer tools. Recognizing critical features, such as safety and price, reflects the industry's priorities in vehicle design.

This project aims to develop a classification model to predict car acceptability based on various features such as price, maintenance cost, safety, and seating capacity. This analysis uses the [Car Evaluation dataset](https://archive.ics.uci.edu/dataset/19/car+evaluation), which contains 1,728 instances and six features: buying price, maintenance cost, number of doors, seating capacity, luggage size, and safety rating. With an increasing number of car models available in the market, understanding how different attributes affect car classification can help streamline the evaluation process. The goal is to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). Being able to predict car acceptability can enhance automated recommendations, improve quality control, and support consumer purchasing decisions.

```{r}
# library(tidyverse)
library(randomForest)  
library(caret) 
library(corrplot) # For correlation heatmap
library(themis) # For SMOTE
library(recipes) # FOR SMOTE
```

```{r}
# this would pull the dataset from original url, extract the zip file and store everything in data directory
# url <- "https://archive.ics.uci.edu/static/public/19/car+evaluation.zip"
# download.file(url, destfile = "car_evaluation.zip")
# unzip("car_evaluation.zip", exdir = "data")
```

```{r}
data <- read.table("data/car.data", header = FALSE, sep = ",")
data|>nrow()
data <- data |> rename(buying = V1,
               maint = V2,
               doors = V3,
               persons = V4, 
               lug_boot = V5,
               safety = V6,
               class = V7)
data$buying <- as.factor(data$buying) 
data$maint <- as.factor(data$maint) 
data$doors <- as.factor(data$doors) 
data$persons <- as.factor(data$persons) 
data$lug_boot <- as.factor(data$lug_boot) 
data$safety <- as.factor(data$safety) 
data$class <- as.factor(data$class) 
```

```{r}
summary(data)
# Check for missing values
colSums(is.na(data))
```

```{r}
# Frequency distribution of the target variable
table(data$class)

# Visualizing the distribution of class
ggplot(data, aes(x = class, fill = class)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Car Evaluations", x = "Class", y = "Count")
```

```{r}
# Visualizing relationships
features <- c("safety", "buying", "persons", "maint", "lug_boot", "doors")
plot_list <- lapply(features, function(feature) {
  ggplot(data, aes(x = .data[[feature]], fill = class)) +
    geom_bar(position = "dodge") +
    theme_minimal() +
    labs(title = paste(feature, "vs. Evaluation Class"))
})

# Display plots separately
for (plot in plot_list) {
  print(plot)
}
```

```{r}
# Ordinal Encoding of Categorical Variables
ordinal_mapping <- list(
  buying = c("low" = 1, "med" = 2, "high" = 3, "vhigh" = 4),
  maint = c("low" = 1, "med" = 2, "high" = 3, "vhigh" = 4),
  doors = c("2" = 2, "3" = 3, "4" = 4, "5more" = 5),
  persons = c("2" = 2, "4" = 4, "more" = 5),
  lug_boot = c("small" = 1, "med" = 2, "big" = 3),
  safety = c("low" = 1, "med" = 2, "high" = 3),
  class = c("unacc" = 1, "acc" = 2, "good" = 3, "vgood" = 4)
)

df_encoded <- data %>% mutate(across(names(ordinal_mapping), ~ ordinal_mapping[[cur_column()]][.]))
```

```{r}
head(df_encoded)
```

```{r}
#There is a great imbalance across different classes, introducing SMOTE to generate synthetic samples in order to improve the distribution
df_encoded$class <- as.factor(df_encoded$class)  # Ensure class is a factor

# Create a recipe for SMOTE
smote_recipe <- recipe(class ~ ., data = df_encoded) %>%
  step_smote(class, over_ratio = 1) %>%
  prep() %>%
  bake(new_data = NULL)

df_balanced <- smote_recipe

# Visualizing the distribution of class after SMOTE 
ggplot(df_balanced, aes(x = class, fill = class)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Car Evaluations After Feature Engineering", x = "Class", y = "Count")
```

```{r}
# Visualizing relationships
features <- c("safety", "buying", "persons", "maint", "lug_boot", "doors")
plot_list <- lapply(features, function(feature) {
  ggplot(data, aes(x = .data[[feature]], fill = class)) +
    geom_bar(position = "dodge") +
    theme_minimal() +
    labs(title = paste(feature, "vs. Evaluation Class"))
})

# Display plots separately
for (plot in plot_list) {
  print(plot)
}
```

```{r}
# Correlation Heatmap
corr_matrix <- cor(df_balanced %>% mutate(across(where(is.factor), as.numeric)))
corrplot(corr_matrix, method = "color", type = "lower", tl.cex = 0.8)
```

```{r}
#Applying Random Forest after encoding and balancing the dataset
n <- nrow(df_balanced )
trainidx <- sample.int(n, floor(n * .75))
testidx <- setdiff(1:n, trainidx)
train <- df_balanced[trainidx, ]
test <- df_balanced[testidx, ]
rf <- randomForest(class ~ ., data = train)
bag <- randomForest(class ~ ., data = train, mtry = ncol(data) - 1)
preds <-  tibble(truth = test$class, rf = predict(rf, test), bag = predict(bag, test))
```

```{r}
predictions <- predict(rf, test)

conf_matrix <- confusionMatrix(predictions, test$class)
conf_matrix
```

## Discussion

Our random forest model achieves accurarcy abot 99% and a strong Kappa statistic about 0.99, which shows it effectively predicts car acceptablility base on the six measurements.

The results we got were very much in line with our expectations as our model performed very well. Our model correctly classifies almost all cars into the correct acceptable categories.

The results we get are very important for decision-making in the automotive industry. Consumers can choose to purchase vehicles based on demand, manufacturers can prioritize the demand for vehicle production, and dealers can develop strategies that are more in line with the market.

Our model is very good, but we still have some interesting questions for the future. Will additional features, such as brand, have an impact on the prediction? Is there any other model that is faster with no loss of accuracy? How is it possible to make our model more realistic? In conclusion, in the future, we may refine the data, compare more models, or try to apply our models to real-world problems.

## References

Bohanec, M. (1988). Car Evaluation \[Dataset\]. UCI Machine Learning Repository. <https://doi.org/10.24432/C5JP48>.

Canada, F. C. A. of. (2024, January 5). Government of Canada. Canada.ca. <https://www.canada.ca/en/financial-consumer-agency/services/loans/financing-car/risks.html>

Chiu, L., Du, J., & Wang, N. (2022). The Effects of Price Dispersion on Sales in the Automobile Industry: A Dynamic Panel Analysis. SAGE Open. <https://doi.org/10.1177/21582440221120647>

Vrkljan, B. H., & Anaby, D. (2011). What vehicle features are considered important when buying an automobile? An examination of driver preferences by age and gender. Journal of Safety Research, 42(1), 61-65. <https://doi.org/10.1016/j.jsr.2010.11.006>
