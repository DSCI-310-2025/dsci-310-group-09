# Car Acceptability Category Prediction Analysis

## Summary

We aim to develop a Random Forest classification model to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). This prediction is based on categorical attributes such as price, maintenance cost, safety, and seating capacity. The data is sourced from the Car Evaluation dataset found in the UCI Machine Learning Repository (Bohanec, 1998).

Our initial data exploration revealed a significant class imbalance, with "unacceptable" cars dominating the dataset. To improve classification performance, we applied ordinal encoding to categorical features and utilized SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.

Our Random Forest model achieved an impressive overall accuracy of 99% and a strong Kappa statistic of 0.99, indicating robust predictive performance and a strong agreement between the predicted and actual classes. Overall, the model provides a highly accurate and interpretable approach to classifying car acceptability, though further refinements could enhance fairness across all categories.

In summary, this analysis demonstrates how machine learning can be effectively applied to real-world decision-making in the automotive sector.

## Introduction

Evaluating car acceptability is a critical factor in decision-making within the automotive industry. It affects consumer choices regarding vehicle purchases, manufacturers' priorities, and dealership strategies. Car purchases represent one of the most significant financial decisions for households, with affordability being the primary barrier for many buyers. Price plays a crucial role in accessibility, particularly for budget-conscious consumers, such as first-time buyers or individuals in emerging markets.

According to Chiu et al. (2022), price dispersion positively impacts car acceptability. Because car purchases are high-cost, long-term investments, misaligned choices due to information asymmetry—such as undervaluing safety features—can lead to serious financial or safety repercussions (Canada, 2024). For instance, vehicles with poor safety ratings have been linked to higher accident rates. Consequently, Vrkljan et al. (2011) concluded that safety is considered the most important feature when purchasing a vehicle. Automating car evaluations helps buyers efficiently identify optimal vehicles, aligning with the trend toward data-driven consumer tools. Recognizing critical features, such as safety and price, reflects the industry's priorities in vehicle design.

This project aims to develop a classification model to predict car acceptability based on various features such as price, maintenance cost, safety, and seating capacity. This analysis uses the [Car Evaluation dataset](https://archive.ics.uci.edu/dataset/19/car+evaluation), which contains 1,728 instances and six features: buying price, maintenance cost, number of doors, seating capacity, luggage size, and safety rating. With an increasing number of car models available in the market, understanding how different attributes affect car classification can help streamline the evaluation process. The goal is to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). Being able to predict car acceptability can enhance automated recommendations, improve quality control, and support consumer purchasing decisions.

## Method & Results

### Install and Loading Libraries

Installs and loads necessary libraries for data processing, visualization, and machine learning. The key libraries include `randomForest` for classification, `caret` for model evaluation, `themis` for handling imbalanced data using SMOTE, and `corrplot`for visualizing correlations.

```{r}
# library(tidyverse)
library(randomForest)  
library(caret) 
library(corrplot) # For correlation heatmap
library(themis) # For SMOTE
library(recipes) # FOR SMOTE
```

```{r}
# this would pull the dataset from original url, extract the zip file and store everything in data directory
# url <- "https://archive.ics.uci.edu/static/public/19/car+evaluation.zip"
# download.file(url, destfile = "car_evaluation.zip")
# unzip("car_evaluation.zip", exdir = "data")
```

### Importing Data & Data Exploration

Reads the dataset, assigns appropriate and meaningful column names, checks the number of rows, and convert all categorical variables into factors. The dataset consists of 1,728 rows with 6 categorical features related to car evaluation.

```{r}
data <- read.table("data/car.data", header = FALSE, sep = ",")
data|>nrow()
data <- data |> rename(buying = V1,
               maint = V2,
               doors = V3,
               persons = V4, 
               lug_boot = V5,
               safety = V6,
               class = V7)
data$buying <- as.factor(data$buying) 
data$maint <- as.factor(data$maint) 
data$doors <- as.factor(data$doors) 
data$persons <- as.factor(data$persons) 
data$lug_boot <- as.factor(data$lug_boot) 
data$safety <- as.factor(data$safety) 
data$class <- as.factor(data$class) 
```

### Summary and Missing Data Check

Provides a summary of each feature, including frequency distributions and potential missing values. It also checks for class imbalance in the target variable `class`.

```{r}
summary(data)
# Check for missing values
colSums(is.na(data))
```

### Class Distribution Visuluzation

Generates a bar chart showing how the car evaluations are distributed across different categories. This helps identify class imbalances.

```{r}
# Frequency distribution of the target variable
table(data$class)

# Visualizing the distribution of class
ggplot(data, aes(x = class, fill = class)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Car Evaluations", x = "Class", y = "Count")
```

The dataset shows significant class imbalance, with the unacc (unacceptable) class dominating at 71% of the original data (1,210 out of 1,728 instances). This imbalance may skew model predictions, leading to poor performance on the minority classes (good, vgood).

### Feature Relationships Visualization

Creates bar charts for each feature to analyze its relationship with the target variable `class`. This helps identify important factors influencing car evaluations.

```{r}
# Visualizing relationships
features <- c("safety", "buying", "persons", "maint", "lug_boot", "doors")
plot_list <- lapply(features, function(feature) {
  ggplot(data, aes(x = .data[[feature]], fill = class)) +
    geom_bar(position = "dodge") +
    theme_minimal() +
    labs(title = paste(feature, "vs. Evaluation Class"))
})

# Display plots separately
for (plot in plot_list) {
  print(plot)
}
```

### Feature Engineering and Data Resampling

Encodes categorical variables into numeric values based on an ordinal scale to make them suitable for Random Forest classification model.

```{r}
# Ordinal Encoding of Categorical Variables
ordinal_mapping <- list(
  buying = c("low" = 1, "med" = 2, "high" = 3, "vhigh" = 4),
  maint = c("low" = 1, "med" = 2, "high" = 3, "vhigh" = 4),
  doors = c("2" = 2, "3" = 3, "4" = 4, "5more" = 5),
  persons = c("2" = 2, "4" = 4, "more" = 5),
  lug_boot = c("small" = 1, "med" = 2, "big" = 3),
  safety = c("low" = 1, "med" = 2, "high" = 3),
  class = c("unacc" = 1, "acc" = 2, "good" = 3, "vgood" = 4)
)

df_encoded <- data %>% mutate(across(names(ordinal_mapping), ~ ordinal_mapping[[cur_column()]][.]))
```

```{r}
head(df_encoded)
```

### Handling Class Imbalance Using SMOTE

Applies SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples and balance the dataset, reducing bias in classification.

Visualizes the new class distribution after SMOTE to ensure the dataset is balanced for better model performance.

```{r}
#There is a great imbalance across different classes, introducing SMOTE to generate synthetic samples in order to improve the distribution
df_encoded$class <- as.factor(df_encoded$class)  # Ensure class is a factor

# Create a recipe for SMOTE
smote_recipe <- recipe(class ~ ., data = df_encoded) %>%
  step_smote(class, over_ratio = 1) %>%
  prep() %>%
  bake(new_data = NULL)

df_balanced <- smote_recipe

# Visualizing the distribution of class after SMOTE 
ggplot(df_balanced, aes(x = class, fill = class)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Distribution of Car Evaluations After Feature Engineering", x = "Class", y = "Count")
```

```{r}
# Visualizing relationships
features <- c("safety", "buying", "persons", "maint", "lug_boot", "doors")
plot_list <- lapply(features, function(feature) {
  ggplot(data, aes(x = .data[[feature]], fill = class)) +
    geom_bar(position = "dodge") +
    theme_minimal() +
    labs(title = paste(feature, "vs. Evaluation Class"))
})

# Display plots separately
for (plot in plot_list) {
  print(plot)
}
```

### Correlation Heatmap

Generates a correlation heatmap to examine relationships between features. Helps identify redundant or highly correlated features.

```{r}
# Correlation Heatmap
corr_matrix <- cor(df_balanced %>% mutate(across(where(is.factor), as.numeric)))
corrplot(corr_matrix, method = "color", type = "lower", tl.cex = 0.8)
```

### Classification Model

1.  Splits the balanced dataset into 75% training and 25% testing sets for model training and evaluation.

2.  Trains two models:

-   `Random Forest (rf)`: Uses multiple decision trees with feature selection.

-   `Bagging (bag)`: Uses all features to grow trees, reducing variance.

3.  Predicts class labels for the test set and computes a confusion matrix to evaluate accuracy, sensitivity, and specificity of the model.

```{r}
#Applying Random Forest after encoding and balancing the dataset
n <- nrow(df_balanced )
trainidx <- sample.int(n, floor(n * .75))
testidx <- setdiff(1:n, trainidx)
train <- df_balanced[trainidx, ]
test <- df_balanced[testidx, ]
rf <- randomForest(class ~ ., data = train)
bag <- randomForest(class ~ ., data = train, mtry = ncol(data) - 1)
preds <-  tibble(truth = test$class, rf = predict(rf, test), bag = predict(bag, test))
```

```{r}
predictions <- predict(rf, test)

conf_matrix <- confusionMatrix(predictions, test$class)
conf_matrix
```

## Discussion

## References

Bohanec, M. (1988). Car Evaluation \[Dataset\]. UCI Machine Learning Repository. <https://doi.org/10.24432/C5JP48>.

Canada, F. C. A. of. (2024, January 5). Government of Canada. Canada.ca. <https://www.canada.ca/en/financial-consumer-agency/services/loans/financing-car/risks.html>

Chiu, L., Du, J., & Wang, N. (2022). The Effects of Price Dispersion on Sales in the Automobile Industry: A Dynamic Panel Analysis. SAGE Open. <https://doi.org/10.1177/21582440221120647>

Vrkljan, B. H., & Anaby, D. (2011). What vehicle features are considered important when buying an automobile? An examination of driver preferences by age and gender. Journal of Safety Research, 42(1), 61-65. <https://doi.org/10.1016/j.jsr.2010.11.006>
