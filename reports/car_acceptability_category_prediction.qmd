---
title: "Car Acceptability Category Prediction Analysis"
author: "Jiaming Change, Yicheng Huang, Effie Wang, & Brianna Zhou"
format: 
  html:
    number-figures: true
    number-tables: true
    embed-resources: true
editor: source
bibliography: references.bib
toc: true
execute:
   echo: false
   warning: false
---

## Summary

We aim to develop a Random Forest classification model to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). This prediction is based on categorical attributes such as price, maintenance cost, safety, and seating capacity. The data is sourced from the Car Evaluation dataset found in the UCI Machine Learning Repository [@Bohanec1988].

Our initial data exploration revealed a significant class imbalance, with "unacceptable" cars dominating the dataset. To improve classification performance, we applied ordinal encoding to categorical features and utilized SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.

Our Random Forest model achieved an impressive overall accuracy of 99% and a strong Kappa statistic of 0.99, indicating robust predictive performance and a strong agreement between the predicted and actual classes. Overall, the model provides a highly accurate and interpretable approach to classifying car acceptability, though further refinements could enhance fairness across all categories.

In summary, this analysis demonstrates how machine learning can be effectively applied to real-world decision-making in the automotive sector.

## Introduction

Evaluating car acceptability is a critical factor in decision-making within the automotive industry. It affects consumer choices regarding vehicle purchases, manufacturers' priorities, and dealership strategies. Car purchases represent one of the most significant financial decisions for households, with affordability being the primary barrier for many buyers. Price plays a crucial role in accessibility, particularly for budget-conscious consumers, such as first-time buyers or individuals in emerging markets.

According to @Chiu2022, price dispersion positively impacts car acceptability. Because car purchases are high-cost, long-term investments, misaligned choices due to information asymmetry—such as undervaluing safety features—can lead to serious financial or safety repercussions [@Canada2024]. For instance, vehicles with poor safety ratings have been linked to higher accident rates. Consequently, @Vrkljan2011 concluded that safety is considered the most important feature when purchasing a vehicle. Automating car evaluations helps buyers efficiently identify optimal vehicles, aligning with the trend toward data-driven consumer tools. Recognizing critical features, such as safety and price, reflects the industry's priorities in vehicle design.

This project aims to develop a classification model to predict car acceptability based on various features such as price, maintenance cost, safety, and seating capacity. This analysis uses the [Car Evaluation dataset](https://archive.ics.uci.edu/dataset/19/car+evaluation), which contains 1,728 instances and six features: buying price, maintenance cost, number of doors, seating capacity, luggage size, and safety rating. With an increasing number of car models available in the market, understanding how different attributes affect car classification can help streamline the evaluation process. The goal is to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). Being able to predict car acceptability can enhance automated recommendations, improve quality control, and support consumer purchasing decisions.

## Method & Results

```{r}
#library(tidyverse)
library(knitr)
```

### Importing Data & Data Exploration

Reads the dataset, assigns appropriate and meaningful column names, checks the number of rows, and convert all categorical variables into factors. The dataset consists of 1,728 rows with 6 categorical features related to car evaluation. 

```{r}
#| label: tbl-cleaned_data
#| tbl-cap: Cleaned Data
cleaned_data <- readRDS("../data/cleaned/cleaned.RDS")
knitr::kable(head(cleaned_data,5))

```

### Summary and Missing Data Check

Provides a summary of each feature, including frequency distributions and potential missing values. It also checks for class imbalance in the target variable `class`. There is no missing value in attributes.
```{r}
summary(cleaned_data)
```

### Class Distribution Visualization

Generates a bar chart showing how the car evaluations are distributed across different categories. This helps identify class imbalances.

```{r}
#| label: tbl-frequency_distri_car  
#| tbl-cap: Frequency distribution of the Car Acceptability Category.

freq_table <- readRDS("../output/tbl_target_dist.RDS")
knitr::kable(freq_table, col.names = c("class", "Frequency"))

```


![Distribution of Car Evaluations](../output/fig_target_dist.png){#fig-car_distribution_plot}

The dataset shows significant class imbalance as shown in @tbl-frequency_distri_car and @fig-car_distribution_plot, with the `unacc` (unacceptable) class dominating at 71% of the original data (1,210 out of 1,728 instances). This imbalance may skew model predictions, leading to poor performance on the minority classes (`good`, `vgood`).

### Feature Relationships Visualization

Creates bar charts for each feature to analyze its relationship with the target variable `class`. This helps identify important factors influencing car evaluations.


![Safety vs. Evaluation Class](../output/fig_relation_safety_1.png){#fig-car_safety_plot}

![Buying vs. Evaluation Class](../output/fig_relation_buying_1.png){#fig-car_buying_plot}

![Number of Passenger vs. Evaluation Class](../output/fig_relation_persons_1.png){#fig-car_persons_plot}

![Maintenance vs. Evaluation Class](../output/fig_relation_maint_1.png){#fig-car_maintenance_plot}

![Luggage Boot Size vs. Evaluation Class](../output/fig_relation_lug_boot_1.png){#fig-car_luggage_plot}

![Number of Doors vs. Evaluation Class](../output/fig_relation_doors_1.png){#fig-car_doors_plot}


The analysis of various features—such as safety @fig-car_safety_plot, buying price @fig-car_buying_plot, passenger capacity @fig-car_persons_plot, maintenance cost @fig-car_maintenance_plot, luggage boot size @fig-car_luggage_plot, and number of doors—shows @fig-car_doors_plot that the dominant classification is `unacc` (unacceptable), which indicates a pattern of negative evaluations.

There is a strong correlation between high safety ratings and the classifications `vgood` (very good) and "good". In contrast, vehicles that have low to medium safety ratings, along with high buying and maintenance costs, are often classified as `unacc`.

Affordable cars, particularly those with low to medium prices, and vehicles with larger luggage boot sizes tend to have higher proportions of `acc` (acceptable) or `vgood` ratings. Conversely, cars with larger passenger capacities or smaller boots are frequently labeled as `unacc`.

The number of doors appears to have a minimal impact on the evaluations, as `unacc` remains the most common classification regardless of this feature.

### Feature Engineering and Data Resampling

Encodes categorical variables into numeric values based on an ordinal scale to make them suitable for Random Forest classification model as shown in @tbl-encoded_variable.

```{r}
#| label: tbl-encoded_variable
#| tbl-cap: Ordinal Encoding of Categorical Variables.

encoded_variable_table <- readRDS("../output/encoded.RDS")
knitr::kable(head(encoded_variable_table, 5))
```

### Handling Class Imbalance Using SMOTE

There is a great imbalance across different classes, introducing SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples in order, balance the dataset, reducing bias in classification, which improves the distribution.

![Distribution of Car Evaluations After Feature Engineering](../output/fig_smote_dist.png){#fig-car_distri_after_enginerring_plot}

Visualizes the new class distribution after SMOTE to ensure the dataset is balanced for better model performance. In contrast to the original imbalanced dataset, where `unacc` dominated with 1,210 instances, all classes (`unacc`, `acc`, `good`, `vgood`) now have approximately equal counts of around 1,250 each as shown in @fig-car_distri_after_enginerring_plot. This reflects the effectiveness of synthetic oversampling (SMOTE) in addressing the severe class imbalance.

### Visualizing relationships with Class Balanced

![Safety vs. Evaluation Class with Class Balance](../output/fig_relation_safety_2.png){#fig-car_safety_balance_plot}

![Buying vs. Evaluation Class with Class Balance](../output/fig_relation_buying_2.png){#fig-car_buying_balance_plot}

![Number of Passenger vs. Evaluation Class with Class Balance](../output/fig_relation_persons_2.png){#fig-car_persons_balance_plot}

![Maintenance vs. Evaluation Class with Class Balance](../output/fig_relation_maint_2.png){#fig-car_maintenance_balance_plot}

![Luggage Boot Size vs. Evaluation Class with Class Balance](../output/fig_relation_lug_boot_2.png){#fig-car_luggage_balance_plot}

![Number of Doors vs. Evaluation Class with Class Balance](../output/fig_relation_doors_2.png){#fig-car_doors_balance_plot}

The "safety vs. Evaluation Class" plot @fig-car_safety_balance_plot shows that cars with high safety ratings mostly receive `vgood`/`good` evaluations, while `low`/`med` safety cars are often labeled `unacc`. In "buying vs. Evaluation Class" @fig-car_buying_balance_plot, higher prices (`high`, `vhigh`) correlate with `unacc`, while affordable cars (`low`, `med`) are more likely to be `acc`/`vgood`. The "Number of Passenger vs. Evaluation Class" @fig-car_persons_balance_plot. indicates that cars with more passenger capacity tend to be `unacc`due to impracticality. For "maint vs. Evaluation Class" @fig-car_maintenance_balance_plot, high maintenance costs link to `unacc`, while lower costs improve acceptability. Lastly, door count has a minimal impact as shown in @fig-car_doors_balance_plot, with `unacc` prevailing across categories, though cars with 5+ doors show slightly higher `acc`/`vgood`. Overall, `unacc` is associated with high-cost, low-safety, or impractical configurations, while `vgood`/`good` relate to affordability, safety, and utility.

### Correlation Heatmap

![Correlation Heatmap](../output/fig_corr_heatmap.png){#fig-corr_heatmap}

Generates a correlation heatmap to examine relationships between features. Helps identify redundant or highly correlated features. This visualization @fig-corr_heatmap illustrates the correlation and feature importance of various factors that influence car evaluation outcomes. The values range from -1, indicating a strong negative association, to +1, which indicates a strong positive association.

The strongest predictors are safety ratings and buying prices. Higher safety ratings are strongly associated with more favorable evaluations (such as "very good" or "good"), while cars with high or very high buying prices tend to receive negative outcomes (such as "unacceptable").

Maintenance costs and the number of doors have a moderate influence, with lower maintenance costs enhancing acceptability. In contrast, features like luggage boot size and passenger capacity show minimal impact on car evaluations.

### Classification Model

1.  Splits the balanced dataset into 75% training and 25% testing sets for model training and evaluation.

2.  Trains two models:

-   `Random Forest (rf)`: Uses multiple decision trees with feature selection.

-   `Bagging (bag)`: Uses all features to grow trees, reducing variance.

3.  Predicts class labels for the test set and computes a confusion matrix to evaluate accuracy, sensitivity, and specificity of the model.


```{r}
#| label: tbl-overall_confusion_matrix
#| tbl-cap: Confusion Matrix of Random Forest Model

confusion_matrix <- readRDS("../output/matrix.RDS")
knitr::kable(confusion_matrix$overall)
knitr::kable(confusion_matrix$byClass)

accuracy <- round(confusion_matrix$overall["Accuracy"], 3)
accuracy_percentage <- accuracy*100
kappa <- round(confusion_matrix$overall["Kappa"], 3)

```


The confusion matrix summarizes the performance of the Random Forest model by showing the number of correct and incorrect predictions across different classes. It provides insights into how well the model distinguishes between categories by comparing actual vs. predicted values as shown in @tbl-overall_confusion_matrix. In addition, it provides per-class evaluation metrics, giving detailed insights into how well the model performs for each individual class.

The model achieved an overall accuracy approximately of `{r} accuracy`, correctly classifying cars in most cases. The approximate Kappa score `{r} kappa` indicates a strong agreement between predictions and actual labels, while the high sensitivity and specificity values demonstrate excellent detection ability across all classes.

### Result Visualization

We visualize the confusion matrix to get a better idea of the accuracy of the model.

```{r}
#| label: tbl-confusion_matrix_table
#| tbl-cap: Confusion Matrix of Random Forest Model

confusion_matrix <- readRDS("../output/matrix.RDS")
knitr::kable(confusion_matrix$table)
```

![Confusion Matrix Heatmap](../output/fig_conf_matrix.png){#fig-confusion_maxtrix__heatmap}

In @fig-confusion_maxtrix__heatmap and @tbl-confusion_matrix_table, the diagonal elements represent correct classifications, which means the model accurately predicted those classes of car. The off-diagonal elements represent incorrect classifications, which shows errors in the model made. Most of the predictions fall on the diagonal line, suggesting that the model performs very well in predicting the dataset. There are few cases of misclassification, which means that the accuracy rate is high.

## Discussion

Our random forest model achieves approximate accuracy of `{r} accuracy_percentage`% and a strong Kappa statistic about `{r} kappa`, which shows it effectively predicts car acceptability base on the six measurements.

The results we got were very much in line with our expectations as our model performed very well. Our model correctly classifies almost all cars into the correct acceptable categories.

The results we get are very important for decision-making in the automotive industry. Consumers can choose to purchase vehicles based on demand, manufacturers can prioritize the demand for vehicle production, and dealers can develop strategies that are more in line with the market.

Our model is very good, but we still have some interesting questions for the future. Will additional features, such as brand, have an impact on the prediction? Is there any other model that is faster with no loss of accuracy? How is it possible to make our model more realistic? In conclusion, in the future, we may refine the data, compare more models, or try to apply our models to real-world problems.

## References
