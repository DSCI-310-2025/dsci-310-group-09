---
title: "Car Acceptability Category Prediction Analysis"
author: "Jiaming Change, Yicheng Huang, Effie Wang, Brianna Zhou"
format: 
  html:
    number-figures: true
    number-tables: true
    embed-resources: true
editor: source
bibliography: references.bib
toc: true
execute:
   echo: false
   warning: false
---

## Summary

We aim to develop a Random Forest classification model to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). This prediction is based on categorical attributes such as price, maintenance cost, safety, and seating capacity. The data is sourced from the Car Evaluation dataset found in the UCI Machine Learning Repository [@Bohanec1988].

Our initial data exploration revealed a significant class imbalance, with "unacceptable" cars dominating the dataset. To improve classification performance, we applied ordinal encoding to categorical features and utilized SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.

Our Random Forest model achieved an impressive overall accuracy of 99% and a strong Kappa statistic of 0.99, indicating robust predictive performance and a strong agreement between the predicted and actual classes. Overall, the model provides a highly accurate and interpretable approach to classifying car acceptability, though further refinements could enhance fairness across all categories.

In summary, this analysis demonstrates how machine learning can be effectively applied to real-world decision-making in the automotive sector.

## Introduction

Evaluating car acceptability is a critical factor in decision-making within the automotive industry. It affects consumer choices regarding vehicle purchases, manufacturers' priorities, and dealership strategies. Car purchases represent one of the most significant financial decisions for households, with affordability being the primary barrier for many buyers. Price plays a crucial role in accessibility, particularly for budget-conscious consumers, such as first-time buyers or individuals in emerging markets.

According to @Chiu2022, price dispersion positively impacts car acceptability. Because car purchases are high-cost, long-term investments, misaligned choices due to information asymmetry—such as undervaluing safety features—can lead to serious financial or safety repercussions [@Canada2024]. For instance, vehicles with poor safety ratings have been linked to higher accident rates. Consequently, @Vrkljan2011 concluded that safety is considered the most important feature when purchasing a vehicle. Automating car evaluations helps buyers efficiently identify optimal vehicles, aligning with the trend toward data-driven consumer tools. Recognizing critical features, such as safety and price, reflects the industry's priorities in vehicle design.

This project aims to develop a classification model to predict car acceptability based on various features such as price, maintenance cost, safety, and seating capacity. This analysis uses the [Car Evaluation dataset](https://archive.ics.uci.edu/dataset/19/car+evaluation), which contains 1,728 instances and six features: buying price, maintenance cost, number of doors, seating capacity, luggage size, and safety rating. With an increasing number of car models available in the market, understanding how different attributes affect car classification can help streamline the evaluation process. The goal is to predict car acceptability categories: unacceptable (unacc), acceptable (acc), good (good), and very good (vgood). Being able to predict car acceptability can enhance automated recommendations, improve quality control, and support consumer purchasing decisions.

## Method & Results

### Install and Loading Libraries

Installs and loads necessary libraries for data processing, visualization, and machine learning. The key libraries include `randomForest` for classification, `caret` for model evaluation, `themis` for handling imbalanced data using SMOTE, and `corrplot`for visualizing correlations.

```{r}
# library(tidyverse)
library(randomForest)  
library(caret) 
library(corrplot) # For correlation heatmap
library(themis) # For SMOTE
library(recipes) # FOR SMOTE
```

```{r}
# this would pull the dataset from original url, extract the zip file and store everything in data directory
# url <- "https://archive.ics.uci.edu/static/public/19/car+evaluation.zip"
# download.file(url, destfile = "car_evaluation.zip")
# unzip("car_evaluation.zip", exdir = "data")
```

### Importing Data & Data Exploration

Reads the dataset, assigns appropriate and meaningful column names, checks the number of rows, and convert all categorical variables into factors. The dataset consists of 1,728 rows with 6 categorical features related to car evaluation.

```{r}
data <- read.table("data/car.data", header = FALSE, sep = ",")
data|>nrow()
data <- data |> rename(buying = V1,
               maint = V2,
               doors = V3,
               persons = V4, 
               lug_boot = V5,
               safety = V6,
               class = V7)
data$buying <- as.factor(data$buying) 
data$maint <- as.factor(data$maint) 
data$doors <- as.factor(data$doors) 
data$persons <- as.factor(data$persons) 
data$lug_boot <- as.factor(data$lug_boot) 
data$safety <- as.factor(data$safety) 
data$class <- as.factor(data$class) 
```

### Summary and Missing Data Check

Provides a summary of each feature, including frequency distributions and potential missing values. It also checks for class imbalance in the target variable `class`. There is no missing value in attributes.

```{r}
summary(data)
# Check for missing values
colSums(is.na(data))
```

### Class Distribution Visualization

Generates a bar chart showing how the car evaluations are distributed across different categories. This helps identify class imbalances.

```{r}
# Frequency distribution of the target variable
table(data$class)

# Visualizing the distribution of class
ggplot(data, aes(x = class, fill = class)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Figure 1: Distribution of Car Evaluations", x = "Class", y = "Count")
```

The dataset shows significant class imbalance, with the `unacc` (unacceptable) class dominating at 71% of the original data (1,210 out of 1,728 instances). This imbalance may skew model predictions, leading to poor performance on the minority classes (`good`, `vgood`).

### Feature Relationships Visualization

Creates bar charts for each feature to analyze its relationship with the target variable `class`. This helps identify important factors influencing car evaluations.

```{r}
# Visualizing relationships
features <- c("safety", "buying", "persons", "maint", "lug_boot", "doors")
plot_list <- lapply(features, function(feature) {
  ggplot(data, aes(x = .data[[feature]], fill = class)) +
    geom_bar(position = "dodge") +
    theme_minimal() +
    labs(title = paste("Figure 2 - 7:", feature, "vs. Evaluation Class"))
})

# Display plots separately
for (plot in plot_list) {
  print(plot)
}
```

The analysis of various features—such as safety, buying price, passenger capacity, maintenance cost, luggage boot size, and number of doors—shows that the dominant classification is `unacc` (unacceptable), which indicates a pattern of negative evaluations.

There is a strong correlation between high safety ratings and the classifications `vgood` (very good) and "good." In contrast, vehicles that have low to medium safety ratings, along with high buying and maintenance costs, are often classified as `unacc`.

Affordable cars, particularly those with low to medium prices, and vehicles with larger luggage boot sizes tend to have higher proportions of `acc` (acceptable) or `vgood` ratings. Conversely, cars with larger passenger capacities or smaller boots are frequently labeled as `unacc`.

The number of doors appears to have a minimal impact on the evaluations, as `unacc` remains the most common classification regardless of this feature.

### Feature Engineering and Data Resampling

Encodes categorical variables into numeric values based on an ordinal scale to make them suitable for Random Forest classification model.

```{r}
# Ordinal Encoding of Categorical Variables
ordinal_mapping <- list(
  buying = c("low" = 1, "med" = 2, "high" = 3, "vhigh" = 4),
  maint = c("low" = 1, "med" = 2, "high" = 3, "vhigh" = 4),
  doors = c("2" = 2, "3" = 3, "4" = 4, "5more" = 5),
  persons = c("2" = 2, "4" = 4, "more" = 5),
  lug_boot = c("small" = 1, "med" = 2, "big" = 3),
  safety = c("low" = 1, "med" = 2, "high" = 3),
  class = c("unacc" = 1, "acc" = 2, "good" = 3, "vgood" = 4)
)

df_encoded <- data %>% mutate(across(names(ordinal_mapping), ~ ordinal_mapping[[cur_column()]][.]))
```

```{r}
head(df_encoded)
```

### Handling Class Imbalance Using SMOTE

Applies SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples and balance the dataset, reducing bias in classification.

Visualizes the new class distribution after SMOTE to ensure the dataset is balanced for better model performance.

```{r}
#There is a great imbalance across different classes, introducing SMOTE to generate synthetic samples in order to improve the distribution
df_encoded$class <- as.factor(df_encoded$class)  # Ensure class is a factor

# Create a recipe for SMOTE
smote_recipe <- recipe(class ~ ., data = df_encoded) %>%
  step_smote(class, over_ratio = 1) %>%
  prep() %>%
  bake(new_data = NULL)

df_balanced <- smote_recipe

# Visualizing the distribution of class after SMOTE 
ggplot(df_balanced, aes(x = class, fill = class)) +
  geom_bar() +
  theme_minimal() +
  labs(title = "Figure 8: Distribution of Car Evaluations After Feature Engineering", x = "Class", y = "Count")
```

In contrast to the original imbalanced dataset, where `unacc` dominated with 1,210 instances, all classes (`unacc`, `acc`, `good`, `vgood`) now have approximately equal counts of around 1,250 each. This reflects the effectiveness of synthetic oversampling (SMOTE) in addressing the severe class imbalance.

```{r}
# Visualizing relationships
features <- c("safety", "buying", "persons", "maint", "lug_boot", "doors")
plot_list <- lapply(features, function(feature) {
  ggplot(data, aes(x = .data[[feature]], fill = class)) +
    geom_bar(position = "dodge") +
    theme_minimal() +
    labs(title = paste("Figure 9 - 14:",feature, "vs. Evaluation Class"))
})

# Display plots separately
for (plot in plot_list) {
  print(plot)
}
```

The "safety vs. Evaluation Class" plot shows that cars with high safety ratings mostly receive `vgood`/`good` evaluations, while `low`/`med` safety cars are often labeled `unacc`. In "buying vs. Evaluation Class," higher prices (`high`, `vhigh`) correlate with `unacc`, while affordable cars (`low`, `med`) are more likely to be `acc`/`vgood`. The "persons vs. Evaluation Class" indicates that cars with more passenger capacity tend to be `unacc`due to impracticality. For "maint vs. Evaluation Class," high maintenance costs link to `unacc`, while lower costs improve acceptability. Lastly, door count has a minimal impact, with `unacc` prevailing across categories, though cars with 5+ doors show slightly higher `acc`/`vgood`. Overall, `unacc` is associated with high-cost, low-safety, or impractical configurations, while `vgood`/`good` relate to affordability, safety, and utility.

### Correlation Heatmap

Generates a correlation heatmap to examine relationships between features. Helps identify redundant or highly correlated features.

```{r}
# Correlation Heatmap
corr_matrix <- cor(df_balanced %>% mutate(across(where(is.factor), as.numeric)))
corrplot(corr_matrix, method = "color", type = "lower", tl.cex = 0.8)
title("Figure 15: Correlation Heatmap")
```

This visualization illustrates the correlation and feature importance of various factors that influence car evaluation outcomes. The values range from -1, indicating a strong negative association, to +1, which indicates a strong positive association.

The strongest predictors are safety ratings and buying prices. Higher safety ratings are strongly associated with more favorable evaluations (such as "very good" or "good"), while cars with high or very high buying prices tend to receive negative outcomes (such as "unacceptable").

Maintenance costs and the number of doors have a moderate influence, with lower maintenance costs enhancing acceptability. In contrast, features like luggage boot size and passenger capacity show minimal impact on car evaluations.

### Classification Model

1.  Splits the balanced dataset into 75% training and 25% testing sets for model training and evaluation.

2.  Trains two models:

-   `Random Forest (rf)`: Uses multiple decision trees with feature selection.

-   `Bagging (bag)`: Uses all features to grow trees, reducing variance.

3.  Predicts class labels for the test set and computes a confusion matrix to evaluate accuracy, sensitivity, and specificity of the model.

```{r}
#Applying Random Forest after encoding and balancing the dataset
n <- nrow(df_balanced )
trainidx <- sample.int(n, floor(n * .75))
testidx <- setdiff(1:n, trainidx)
train <- df_balanced[trainidx, ]
test <- df_balanced[testidx, ]
rf <- randomForest(class ~ ., data = train)
bag <- randomForest(class ~ ., data = train, mtry = ncol(data) - 1)
preds <-  tibble(truth = test$class, rf = predict(rf, test), bag = predict(bag, test))
```

```{r}
predictions <- predict(rf, test)

conf_matrix <- confusionMatrix(predictions, test$class)
conf_matrix
```

The model achieved an overall accuracy of approximately 99%, correctly classifying cars in most cases. The Kappa score of approximately 0.99 indicates a strong agreement between predictions and actual labels, while the high sensitivity and specificity values demonstrate excellent detection ability across all classes.

### Result Visualization

We visualize the confusion matrix to get a better idea of the accuracy of the model.

```{R}
# Convert confusion matrix to data frame
conf_df <- data.frame(conf_matrix$table)

# Plot Confusion Matrix as Heatmap
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "black", size = 5) +
  scale_fill_gradient(low = "white", high = "lightblue") +
  labs(title = "Figure 16: Confusion Matrix Heatmap", x = "Predicted Class", y = "Actual Class") +
  theme_minimal()
```

The diagonal elements represent correct classifications, which means the model accurately predicted those classes of car. The off-diagonal elements represent incorrect classifications, which shows errors in the model made. Most of the predictions fall on the diagonal line, suggesting that the model performs very well in predicting the dataset. There are few cases of misclassification, which means that the accuracy rate is high.

## Discussion

Our random forest model achieves accuracy about 99% and a strong Kappa statistic about 0.99, which shows it effectively predicts car acceptability base on the six measurements.

The results we got were very much in line with our expectations as our model performed very well. Our model correctly classifies almost all cars into the correct acceptable categories.

The results we get are very important for decision-making in the automotive industry. Consumers can choose to purchase vehicles based on demand, manufacturers can prioritize the demand for vehicle production, and dealers can develop strategies that are more in line with the market.

Our model is very good, but we still have some interesting questions for the future. Will additional features, such as brand, have an impact on the prediction? Is there any other model that is faster with no loss of accuracy? How is it possible to make our model more realistic? In conclusion, in the future, we may refine the data, compare more models, or try to apply our models to real-world problems.

## References

Bohanec, M. (1988). Car Evaluation \[Dataset\]. UCI Machine Learning Repository. <https://doi.org/10.24432/C5JP48>.

Canada, F. C. A. of. (2024, January 5). Government of Canada. Canada.ca. <https://www.canada.ca/en/financial-consumer-agency/services/loans/financing-car/risks.html>

Chiu, L., Du, J., & Wang, N. (2022). The Effects of Price Dispersion on Sales in the Automobile Industry: A Dynamic Panel Analysis. SAGE Open. <https://doi.org/10.1177/21582440221120647>

Vrkljan, B. H., & Anaby, D. (2011). What vehicle features are considered important when buying an automobile? An examination of driver preferences by age and gender. Journal of Safety Research, 42(1), 61-65. <https://doi.org/10.1016/j.jsr.2010.11.006>
